# 爬虫的一般思路——以爬取“‘快代理’网页,构建ip代理池”为例
    1.分析目标网页，确定爬取的url路径、headers参数等
    2.发送请求 —— requests 模拟浏览器发送请求，获取响应数据
    3.解析数据 —— parsel 转化为Selector对象，Selector对象具有xpath方法，能够对转化的数据进行处理
    4.保存数据
---
## 1.分析目标网页，确定爬取的url路径、headers参数等
分析动/静态网页方法：
>右键查看网页源代码，ctrl+F搜索原网页中的对应内容跟，若在网页源码中，则该网页为静态网页


```python
#获取url路径
base_url = 'https://www.kuaidaili.com/free/'

#headers参数的数据类型为字典
#headers参数具体获取方法：
#   1.在网页中右键进入“检查”（即开发者模式）
#   2.选择“Network”(网络)，刷新
#   3.任意选择一项网页内容，选中“headers”
#   4.提取其中所需元素
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36 Edg/94.0.992.47'}
```
---
## 2.发送请求 —— requests 模拟浏览器发送请求，获取响应数据
```python
#应用requests的方法，首先需要在开头导入requests包
import requests

#在网页headers的General处可查看网页请求方法，如果是GET方式，我们就需要用requests的get方法
#返回响应数据
response = requests.get(base_url,headers=headers)
#print(response.request.headers) #不加headers等于裸奔

#data存储响应数据中的文本信息（即网页源代码）
data = response.text
```
---
## 3.解析数据 —— parsel 转化为Selector对象，Selector对象具有xpath方法，能够对转化的数据进行处理
```python
#代理ip格式：{"协议类型":"ip:端口"}
#在开头导入parsel数据解析模块
import parsel

# 转换数据类型
html_data = parsel.Selector(data)

# 数据解析
# 通过xpath函数进行精确定位
# //符号表示跨节点获取，用@进行精准定位
parse_list = html_data.xpath('//table[@class="table table-bordered table-striped"]/tbody/tr')

# 空列表，存放字典值
proxies_list = []
# 遍历for循环
for tr in parse_list:
    # xpath提取规范，tr数据标签下标从1开始（29:41）
    http_type = tr.xpath('./td[4]/text()').extract_first() # 提取协议类型
    ip_num = tr.xpath('./td[1]/text()').extract_first() # 提取地址
    ip_port = tr.xpath('./td[2]/text()').extract_first() # 提取端口
    # print(http_type,ip_num,ip_port)

    # 构建ip字典
    dict_proxies = {}
    dict_proxies[http_type] = ip_num + ':' + ip_port
    # print(dict_proxies)

    #将各条信息以字典键值对形式存放于列表
    proxies_list.append(dict_proxies)

# print("获取到的ip数量：",len(proxies_list))
# print(proxies_list)
```

---
## 总结
如果想要爬取整个网页，则需要知晓网页翻页规则，将上述代码封装，并进行遍历。

在完成代理ip的爬取之后，针对ip的质量（访问速度）检测也不可或缺：
```python
# ---------------------------------------
def check_ip(proxies_list):
    """检测代理ip质量"""
    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36 Edg/94.0.992.47'}

    can_use = []
    for proxy in proxies_list:
        
        try:
            # 下一条代码揭示了如何使用代理ip
            response = requests.get('https://weibo.com/u/6585424401/home?wvr=5',headers=headers,proxies=proxy,timeout=0.1)# 响应时间超过0.1s，则不合格
            if response.status_code == 200: # 证明访问成功
                can_use.append(proxy)
        except Exception as e:
            print(e)
        finally:
            print('当前ip：',proxy,' 检测通过')
    
    return can_use
# ---------------------------------------
```

---
### 附
json文件的写与读
```python
file_1 = open('1.txt','w+')
for line in can_use:
    file_1.write(json.dumps(line)) # 写入，将每个字典以json格式写入
file_1.close()
```
```python
# 读出，先分割每个字典，存入列表，之后遍历转化
import json
file_2 = open('1.txt','r')
content = file_2.read()
content = content.replace('}{',"}aaa{")
content = content.split('aaa')
for i in content:
    data = json.loads(i)
    print(data)
```